{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "#import codecs\n",
    "#import sys\n",
    "import csv\n",
    "#import requests\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get('http://seec.ct.gov/eCrisReporting/SearchingCommittee.aspx')\n",
    "\n",
    "#creating lambda functions to click our page 'buttons'!\n",
    "clickbyxpath = lambda x: browser.find_element_by_xpath(x).click()\n",
    "clickbyid    = lambda x: browser.find_element_by_id(x).click()\n",
    "clickbytext  = lambda x: browser.find_element_by_link_text(x).click()\n",
    "goback       = lambda: browser.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "\n",
    "StateButton    = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_ddlState\"]/option[16]'\n",
    "OfficeButton   = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_ddlOfficeSought\"]/option[68]'\n",
    "SearchButton   = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_cmdSearch\"]'\n",
    "nextPageButton = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_cmdNext\"]'\n",
    "\n",
    "clickbyxpath(StateButton)\n",
    "clickbyxpath(OfficeButton)\n",
    "clickbyxpath(SearchButton)\n",
    "\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have an issue with the below code, where our while loop, which is intended to scrape the committee details, repeatedly loops through the first page's committes, but not the remaining committees on pages 2-99.\n",
    "### Error resolved 11/07/2017\n",
    "While loop was searching first page repeatedly. I revised by making loop fetch the succeeding page (using pageButton) after the proceeding page was fully parsed\n",
    "\n",
    "11/07/2018 Need to write resulting dictionaries to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row': 1, 'committee_name': 'R  95', 'candidate_chairperson': 'John Carlson', 'treasurer': \"Matthew D O'Brien\", 'dpty_treas': \"James O'Connell\", 'office_sought': 'State Representative', 'committee_type': 'Candidate Committee', 'termination_date': '', 'first_reg_date': '03/14/2018', 'party': 'Republican', 'district': '95', 'city': 'New Haven', 'state': 'CT'}\n",
      "###########################\n",
      "{'timestamp': '10/30/2018 5:13:00 PM ( AMENDMENT )', 'candidate': 'John Carlson', 'committee_name2': 'R  95', 'address1': '291 Greenwich Ave', 'address2': 'New Haven CT 06519 '}\n",
      "###########################\n",
      "{'row': 2, 'committee_name': 'Kraut For Westport', 'candidate_chairperson': 'Greg Kraut', 'treasurer': 'Robert C Bass', 'dpty_treas': '', 'office_sought': 'State Representative', 'committee_type': 'Candidate Committee', 'termination_date': '', 'first_reg_date': '05/23/2018', 'party': 'Republican', 'district': '136', 'city': 'Westport', 'state': 'CT'}\n",
      "###########################\n",
      "{'timestamp': '10/29/2018 12:52:00 PM ( AMENDMENT )', 'candidate': 'Greg Kraut', 'committee_name2': 'Kraut For Westport', 'address1': '60 Center St', 'address2': 'Westport CT 06880 '}\n",
      "###########################\n",
      "{'row': 3, 'committee_name': 'Daniel B Pannone', 'candidate_chairperson': 'Daniel B Pannone', 'treasurer': '', 'dpty_treas': '', 'office_sought': 'State Representative', 'committee_type': 'Candidate Exemption Committee', 'termination_date': '', 'first_reg_date': '10/26/2018', 'party': 'Republican', 'district': '146', 'city': 'Stamford', 'state': 'CT'}\n",
      "###########################\n",
      "{'timestamp': '10/26/2018 12:29:00 PM ( ORIGINAL )', 'candidate': 'Daniel B Pannone', 'committee_name2': 'Daniel B Pannone', 'address1': '100 Akbar Rd', 'address2': 'Stamford CT 06902 '}\n",
      "###########################\n",
      "{'row': 4, 'committee_name': 'Vote Kerouac', 'candidate_chairperson': 'A.J. Kerouac', 'treasurer': 'Kathleen D Jenkins', 'dpty_treas': 'Tina M Kerouack', 'office_sought': 'State Representative', 'committee_type': 'Candidate Committee', 'termination_date': '', 'first_reg_date': '04/02/2018', 'party': 'Republican', 'district': '50', 'city': 'Brooklyn', 'state': 'CT'}\n",
      "###########################\n",
      "{'timestamp': '10/19/2018 12:47:00 PM ( AMENDMENT )', 'candidate': 'A.J. Kerouac', 'committee_name2': 'Vote Kerouac', 'address1': '282 Windham Rd', 'address2': 'Brooklyn CT 06234 '}\n",
      "###########################\n"
     ]
    }
   ],
   "source": [
    "#loops through pages 1,99\n",
    "for pagect in range(1,99):\n",
    "\n",
    "    pageButton = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_ddlGoToPage\"]/option[%d]' % pagect\n",
    "\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "    #isolates first entry of row 1 in table body\n",
    "    #then iterates through each column of the row\n",
    "    #then iterates through subsquent rows\n",
    "    details = 2     \n",
    "\n",
    "################################################################################################################\n",
    "    for rowval in range(1,26):\n",
    "\n",
    "        #this is our beautSoup of rows('tr' tags containing our rows), with 'td' tags being row elements\n",
    "        tableElems = soup.find('table', {'class': 'DataTable_Grid'}).find_all('tr')[rowval].find_all('td')\n",
    "\n",
    "        #define an array\n",
    "        #rowval will be our first element of rowContents.\n",
    "        rowContents = [rowval]      \n",
    "\n",
    "        #define a second list for our detailed committee information\n",
    "        candContents = []\n",
    "        \n",
    "        tableLength = len(tableElems)\n",
    "        \n",
    "##################################################################################################################\n",
    "        #parse columns 1 through 12, one row at a time, for rows 1-25\n",
    "        for col in range(1,tableLength):\n",
    "\n",
    "            column_stuff = tableElems[col].find_all(string=True)\n",
    "            \n",
    "            #join and strip the strings\n",
    "            contents = ''.join(column_stuff).strip()\n",
    "\n",
    "            #append elements from our rows, into our list\n",
    "            rowContents.append(contents)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "        #clicks on candidate\n",
    "        while details<=rowval+1:\n",
    "            \n",
    "            candidateButton = '//*[@id=\"ctl00_ContentPlaceHolder1_UcCommitteeAdvanceSearch1_gvSearchResult\"]/tbody/tr[%d]' % details\n",
    "            clickbyxpath(candidateButton)\n",
    "            \n",
    "            time.sleep(3)\n",
    "\n",
    "            committee_history = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "            explore = committee_history.find_all('tbody')[4].find_all('tr')\n",
    "\n",
    "            cand_name = explore[7].find('tbody')('tr')[7]('td')[8].string\n",
    "            timestamp = explore[7].find_all('td')[7].find('span').string\n",
    "            committee_name2 = explore[7].find_all('td')[12].string\n",
    "            address1 = explore[15].find('tbody')('tr')[3]('td')[1].string\n",
    "            address2 = explore[15].find('tbody')('tr')[4]('td')[1].string\n",
    "\n",
    "            foundstuff = [timestamp, cand_name, committee_name2, address1, address2]\n",
    "            candContents.extend(foundstuff)\n",
    "\n",
    "            goback()\n",
    "            time.sleep(4)\n",
    "            clickbyxpath(StateButton)\n",
    "            clickbyxpath(OfficeButton)\n",
    "            clickbyxpath(SearchButton)\n",
    "            \n",
    "            time.sleep(3)\n",
    "\n",
    "            clickbyxpath(pageButton)            \n",
    "            details+=1        \n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "        firstPage = ['row',\n",
    "                     'committee_name',\n",
    "                     'candidate_chairperson',\n",
    "                     'treasurer',\n",
    "                     'dpty_treas',\n",
    "                     'office_sought',\n",
    "                     'committee_type',\n",
    "                     'termination_date',\n",
    "                     'first_reg_date',\n",
    "                     'party',\n",
    "                     'district',\n",
    "                     'city',\n",
    "                     'state'\n",
    "                     ]\n",
    "\n",
    "        secondPage = ['timestamp',\n",
    "                      'candidate',\n",
    "                      'committee_name2',\n",
    "                      'address1',\n",
    "                      'address2'\n",
    "                      ]\n",
    "\n",
    "        desired_csv1 = dict(zip(firstPage,rowContents))\n",
    "        desired_csv2 = dict(zip(secondPage, candContents))  \n",
    "\n",
    "        print(desired_csv1)\n",
    "        print('###########################')\n",
    "        print(desired_csv2)\n",
    "        print('###########################')\n",
    "\n",
    "\n",
    "clickbyxpath(nextPageButton)\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential method for writing our dictionaries to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_dict_list_to_csv(data, filename):\n",
    "    with open('statereps.csv','w') as f1, open('reps_all.csv', 'w') as f1:\n",
    "\n",
    "        headers1 = sorted([k for k, v in desired_csv1[0].items()])\n",
    "        csv_data1 = [headers1]\n",
    "        \n",
    "        headers = sorted([j for j, w in desired_csv2[0].items()])\n",
    "        csv_data2 = [headers2]\n",
    "\n",
    "        for d in desired_csv1:\n",
    "            csv_data1.append([d[h] for h in headers1])\n",
    "\n",
    "        writer1 = csv.writer(f1)\n",
    "        writer1.writerows(csv_data1)\n",
    "        \n",
    "        for p in desired_csv2:\n",
    "            csv_data2.append([d[h] for h in headers2])\n",
    "            \n",
    "        writer2 = csv.writer(f2)\n",
    "        writer2.writerows(csv_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#write our list of strings to a csv format\n",
    "def writeStuff(csvWriter, foundContent):\n",
    "    csvWriter.writerow(desired_csv1)\n",
    "    csv\n",
    "'''\n",
    "\n",
    "with open('statereps.csv', 'wb') as f1, open('reps_all.csv', 'wb') as f2:\n",
    "    dict_writer1 = csv.DictWriter(csvFile1, keys)\n",
    "    dict_writer2 = csv.DictWriter(csvFile2, keys)\n",
    "\n",
    "    dict_writer1.writeheader()\n",
    "    dict_writer1.writerows(keys1)\n",
    "    for d in r:\n",
    "        dict_writer1.writerow(d)\n",
    "        \n",
    "\n",
    "    dict_writer2.writeheader()\n",
    "    dict_writer.writerows(keys2)\n",
    "    for c in q:\n",
    "        dict_writer1.writerow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-1b171c549b93>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-1b171c549b93>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pd\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    pd\n",
    "    pd.DataFrame(desired_csv1.items())\n",
    "    pd.DataFrame(desired_csv2.items())\n",
    "\n",
    "    pd.DataFrame(d.items(), columns=)\n",
    "\n",
    "    '''\n",
    "    keys1 = desired_csv1.keys()\n",
    "    keys2 = desired_csv2.keys()\n",
    "\n",
    "\n",
    "    with open('statereps.csv', 'w') as csvFile1, open('reps_all.csv', 'w') as csvFile2:\n",
    "        dict_writer1 = csv.DictWriter(csvFile1, keys1)\n",
    "        dict_writer2 = csv.DictWriter(csvFile2, keys2)\n",
    "\n",
    "        dict_writer1.writeheader()\n",
    "        dict_writer1.writerow(desired_csv1)\n",
    "\n",
    "        dict_writer2.writeheader()\n",
    "        dict_writer2.writerow(desired_csv2)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
